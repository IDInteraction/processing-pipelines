---
title: "Depth data analysis"
output:
  pdf_document: default
  html_notebook: default
---
```{r}
library(tidyverse)
```

## Introduction

This notebook contains details of the analysis of the (front) Kinect depth data for the spot the difference experiment.  Depth data were extracted from the Kinect xef files using the C# code.  For each Kinect frame we obtain:

* Depth (in mm) for each pixel in the depth camera's reference frame.  These data are 512x484 pixels.
* Depth (in mm) mapped to each pixel in the colour camera's reference frame.  These data are 1920x1080 pixels.  Depth data are not available for some pixels for two reasons:
     + The depth and colour camera's fields of view (and aspect ratios) are different
     + The cameras in in slightly different places, to there is a "shadow" where depth data isn't available

The "colour" depth data is obviously much larger, but contains no additional information.   In general it is easier to use the directly captured depth data rather than the mapped data. The mapped data needs to be used if we're focussing on regions of interest obtained by, e.g. Openface or CppMT.

We can "slice" the depth data to exclude the participant's surroundings, and just focus on their head and body.  The accuracy of the depth data is lower towards the edge of the frame, and appears to be noisier (TODO cite Yahng and Zahng).

I have (so far) considered two approaches to using the depth data:

* Using gaussian mixture models to look at the distribution of depths within each frame.  The mean and standard deviation of each mixture component are passed as inputs to the classifier.
    + The region of interest can be selected using either depth slicing and/or by using data from, e.g. CppMT to focus on the participant's face
* Using princpal component analysis to reduce the dimensionality of the depth data, and using the outputs from this as inputs to the classifier
    + The data can be pre-filtered by depth slicing, to exclude the participant's surroundings.

I also explore the effect of _combining_ the various tracking sources at our disposal (i.e. CppMT, OpenFace and depth PCA data)


### Gaussian mixture models

TODO writeup fully. 

Briefly... take depth readings in region of interest. Fit mixture model.   Need to fix number of components so constant for classifier.  Identifiability issues; ordering by mean not robust to large changes in behaviour.  Essentially didn't really work.


### Principal component analysis

Principal component analysis (PCA) is used to reduce the dimensionality of a data-set.  We use the incremental PCA feature of `sklearn` to extract the first 12 principal components of each frame in each experimental period.  _Incremental_ PCA allows us to batch process the depth data, and so avoid having to load all of the depth data into memory simultaneously.  (The choice of using 12 components was somewhat arbitrary; the performance of the [algorithm scales as $\mathcal{O}(\mathrm{batch size} \times \mathrm{components}^2)$](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA) )

Before performing the PCA analysis we filter each frame to only retain depths between 710 and 1710mm; values outwith this range were set to 0.  These depths were determined using the Shiny app; they retain the majority of the table, and extend as far behind the participant as possible without including the room's walls and ceiling.

The 12 components for each frame are passed to the decision tree classifier.   We repeat the analysis of the DIS2017 abstract using the PCA data instead, which we compare to the webcam CppMT and OpenFace tracking.
```{r, echo = FALSE}

resultsframe <- read.csv("results_all_tracker_combinations.csv")
numparticipants <- length(unique(resultsframe$participantCode))

resultsframe <- as_tibble(resultsframe)
resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart) %>%
  summarise(avgxval = mean(crossvalAccuracy),
            avggt = mean(groundtruthAccuracy),
            datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  stop("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)
```

```{r, echo=FALSE}
singletracker <- stringr::str_count(resultsmean$tracker, ",") == 0 
accuracyfigure <- ggplot(data = resultsmean[singletracker, ], aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_wrap( experimentpart ~ tracker) + xlab("training frames") +
  ylab("mean accuracy") + theme(legend.position = "bottom")
print(accuracyfigure)
```

We see that the PCA approach is outperformed by both the CppMT and OpenFace tracking. It also needs more frames to reach a given level of accuracy than the visual tracking approaches.

### Combining serveral data sources.

In this section we explore the effect of simultaneously using CppMT, OpenFace and DepthPCA data on tracking accuracy.  This is complicated by the fact that data derived from the Kinect data is missing occasional frames, owing to dropouts in the Kinect data stream.  Where this occurs we drop all data for the entire frame.  There are fewer than 15 such frames in any experiment part (and in general far fewer). The charts below show the mean accuracy obtained for each combination of trackers, for each part of the experiment.


```{r, echo = FALSE}

resultsframe <- read.csv("results_all_tracker_combinations.csv")
numparticipants <- length(unique(resultsframe$participantCode))

# Generate coordinates for faceting
# It's easier to do this on character strings, rather than as a factor
resultsframe$tracker <- as.character(resultsframe$tracker)
# Make single source trackers follow the same pattern as the others
onesource <- stringr::str_count(resultsframe$tracker, ",") == 0
resultsframe$tracker <- ifelse(onesource, 
                              paste0("-,", resultsframe$tracker), 
                             resultsframe$tracker)
# 1 way trackers go in first column
# Tokensize trackers
tracktoken <- stringr::str_match(resultsframe$tracker, "([-\\w]+),?(\\w+)?,?(\\w+)?")
resultsframe$facetx <- tracktoken[,2]
resultsframe$facety <- tracktoken[,3]

resultsframe$facetx <- ifelse(!is.na(tracktoken[,4]), "all", resultsframe$facetx)
resultsframe$facety <- ifelse(!is.na(tracktoken[,4]), "all", resultsframe$facety)

resultsframe$facetx <- factor(resultsframe$facetx, levels=c("-", "cppMT", "depthPCA", "all"))
resultsframe$facety <- factor(resultsframe$facety, levels=c("cppMT", "depthPCA", "openface", "all"))

# Print results of encoding to check it worked
#unique(resultsframe[,c("tracker", "facetx", "facety")])

resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart, facetx, facety) %>%
  summarise(avgxval = mean(crossvalAccuracy),
            avggt = mean(groundtruthAccuracy),
            datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  warning("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)


for (part in unique(resultsmean$experimentpart)) {
  accuracyfigure <- ggplot(data = resultsmean[resultsmean$experimentpart == part,],
                           aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_grid( facetx ~ facety) + xlab("training frames") +
  ylab("mean accuracy") + theme(legend.position = "bottom") + ggtitle(part) + ylim(0.5,1)
# We save the figure here - it gets pulled into the document later
#ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")
  print(accuracyfigure)
}


```

It appears that combining several tracking sources leads to improved prediction accuracy.

We can look at these results on a per-participant level; these are in the attached pdf:

```{r, echo = FALSE}

resultsframe$crossvalAccuracy <- ifelse(resultsframe$config == "noshuffle",
                                        NA,
                                        resultsframe$crossvalAccuracy)

#pdf("participantPlots.pdf")
for (part in unique(resultsframe$experimentpart)) {
  for(participant in sort(unique(resultsframe$participantCode))){
    particpantaccuracyfigure <- ggplot(data = resultsframe[resultsframe$experimentpart == part &
                                                   resultsframe$participantCode == participant,],
                             aes(x = frames, y = groundtruthAccuracy, colour = config)) + geom_line() + 
      geom_line(aes(y = crossvalAccuracy), linetype = 2) +   facet_grid( facetx ~ facety) + xlab("training frames") +
      ylab("mean accuracy") + theme(legend.position = "bottom") + ggtitle(paste(participant, part)) + 
      ylim(0.5,1)
    # We save the figure here - it gets pulled into the document later
    #ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")
   print(particpantaccuracyfigure)
  }
}
#dev.off()
```


