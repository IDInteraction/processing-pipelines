---
title: "Depth data analysis"
output:
  pdf_document: default
  html_notebook: default
---
```{r, echo = FALSE, message = FALSE}
library(tidyverse)
yrange = c(0.4,1)
```

## Introduction

This notebook contains details of the analysis of the (front) Kinect depth data for the spot the difference experiment.  Depth data were extracted from the Kinect xef files using the C# code.  For each Kinect frame we obtain:

* Depth (in mm) for each pixel in the depth camera's reference frame.  These data are 512x484 pixels.
* Depth (in mm) mapped to each pixel in the colour camera's reference frame.  These data are 1920x1080 pixels.  Depth data are not available for some pixels for two reasons:
     + The depth and colour camera's fields of view (and aspect ratios) are different
     + The cameras in in slightly different places, so there is a shadow where depth data isn't available

The "colour" depth data is obviously much larger, but contains no additional information.   In general it is easier to use the directly captured depth data rather than the mapped data. The mapped data needs to be used if we're focussing on regions of interest obtained by, e.g. Openface or CppMT.

We can "slice" the depth data to exclude the participant's surroundings, and just focus on their head and body.  The accuracy of the depth data [is lower towards the edge of the frame, and appears to be noisier][yz].



I have (so far) considered two approaches to using the depth data:

* Using Gaussian mixture models to look at the distribution of depths within each frame.  The mean and standard deviation of each mixture component are passed as inputs to the classifier.
    + The region of interest can be selected using either depth slicing and/or by using data from, e.g. CppMT to focus on the participant's face
* Using principal component analysis to reduce the dimensionality of the depth data, and using the outputs from this as inputs to the classifier
    + The data can be pre-filtered by depth slicing, to exclude the participant's surroundings.

I also explore the effect of _combining_ the various tracking sources at our disposal (i.e. CppMT, OpenFace and depth PCA data)


### Gaussian mixture models

(I still need to run this through for all participants. It works better than I initially thought (~75\% accuracy for P01 part 1); I was previously having issues getting the kinect and webcam streams in sync)

In this approach we fit a mixture of Gaussian to the histogram of the depths observed in the frame.  Before doing so we apply depth slicing to exclude the depth readings from the walls and ceiling of the room.

We need to fix the number of Gaussians fitted, so that the same number of covariates are passed for each frame of the classifier; based on visual inspection of a few frames, 4 gaussians were used (it would probably be better to fit a number of different components on a sample of frames and compare model fit via BIC).

The ordering of components from the model is not guaranteed, so we sort components by their mean.  This does not, however, guarantee each component is measuring the same part of the depth histogram; changes in participant pose can cause the model to switch between different configurations of Gaussians (TODO - include a couple of frames showing this; it's difficult to explain in words.)
 
We pass the mean and standard deviation of each component to the classifer as our "tracking" data.

### Principal component analysis

Principal component analysis (PCA) is used to reduce the dimensionality of a data-set.  We use the incremental PCA feature of `sklearn` to extract the first 12 principal components of each frame in each experimental period.  _Incremental_ PCA allows us to batch process the depth data, and so avoid having to load all of the depth data into memory simultaneously.  (The choice of using 12 components was somewhat arbitrary; the performance of the [algorithm scales as $\mathcal{O}(\mathrm{batch size} \times \mathrm{components}^2)$](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA) )

Before performing the PCA analysis we filter each frame to only retain depths between 710 and 1710mm; values outwith this range were set to 0.  These depths were determined using the Shiny app; they retain the majority of the table, and extend as far behind the participant as possible without including the room's walls and ceiling.

The 12 components for each frame are passed to the decision tree classifier.   We repeat the analysis of the DIS2017 abstract using the PCA data instead, which we compare to the webcam CppMT and OpenFace tracking.
```{r, echo = FALSE}

resultsframe <- read.csv("results_all_tracker_combinations.csv")
numparticipants <- length(unique(resultsframe$participantCode))

resultsframe <- as_tibble(resultsframe)
resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart) %>%
  summarise(avgxval = mean(crossvalAccuracy),
            avggt = mean(groundtruthAccuracy),
            datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  stop("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)
```

```{r, echo=FALSE}
singletracker <- stringr::str_count(resultsmean$tracker, ",") == 0 
accuracyfigure <- ggplot(data = resultsmean[singletracker, ], aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_wrap( experimentpart ~ tracker) + xlab("training frames") +
  ylab("mean accuracy") + theme(legend.position = "bottom") + ylim(yrange)
print(accuracyfigure)
```

We see that the PCA approach is outperformed by both the CppMT and OpenFace tracking. It also needs more frames to reach a given level of accuracy than the visual tracking approaches.

### Combining serveral data sources.

In this section we explore the effect of simultaneously using CppMT, OpenFace and DepthPCA data on tracking accuracy.  This is complicated by the fact that data derived from the Kinect data is missing occasional frames, owing to dropouts in the Kinect data stream.  Where this occurs we drop all data for the entire frame.  There are fewer than 15 such frames in any experiment part (and in general far fewer). The charts below show the mean accuracy obtained for each combination of trackers, for each part of the experiment.  The top row shows the performance of each source of tracking data in isolation. The remaining cells show the pairwise combinations of the three sources, with the exception of the lower right cell which shows the performance of all three sources.


```{r, echo = FALSE}

resultsframe <- read.csv("results_all_tracker_combinations.csv")
numparticipants <- length(unique(resultsframe$participantCode))

# Generate coordinates for faceting
# It's easier to do this on character strings, rather than as a factor
resultsframe$tracker <- as.character(resultsframe$tracker)
# Make single source trackers follow the same pattern as the others
onesource <- stringr::str_count(resultsframe$tracker, ",") == 0
resultsframe$tracker <- ifelse(onesource, 
                              paste0("-,", resultsframe$tracker), 
                             resultsframe$tracker)
# 1 way trackers go in first column
# Tokensize trackers
tracktoken <- stringr::str_match(resultsframe$tracker, "([-\\w]+),?(\\w+)?,?(\\w+)?")
resultsframe$facetx <- tracktoken[,2]
resultsframe$facety <- tracktoken[,3]

resultsframe$facetx <- ifelse(!is.na(tracktoken[,4]), "all", resultsframe$facetx)
resultsframe$facety <- ifelse(!is.na(tracktoken[,4]), "all", resultsframe$facety)

resultsframe$facetx <- factor(resultsframe$facetx, levels=c("-", "cppMT", "depthPCA", "all"))
resultsframe$facety <- factor(resultsframe$facety, levels=c("cppMT", "depthPCA", "openface", "all"))

# Print results of encoding to check it worked
#unique(resultsframe[,c("tracker", "facetx", "facety")])

resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart, facetx, facety) %>%
  summarise(avgxval = mean(crossvalAccuracy),
            avggt = mean(groundtruthAccuracy),
            datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  warning("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)


for (part in unique(resultsmean$experimentpart)) {
  accuracyfigure <- ggplot(data = resultsmean[resultsmean$experimentpart == part,],
                           aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_grid( facetx ~ facety) + xlab("training frames") +
  ylab("mean accuracy") + theme(legend.position = "bottom") + ggtitle(part) + ylim(yrange)
# We save the figure here - it gets pulled into the document later
#ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")
  print(accuracyfigure)
}


```

It appears that combining several tracking sources leads to improved prediction accuracy.

I plan to look at the covariates used in the trees to better undestand how the data-sources are being used.  It may be that the depth data is used more in part two of the experiment, where the tracking data is less reliable.


```{r, echo = FALSE, eval = FALSE}

resultsframe$crossvalAccuracy <- ifelse(resultsframe$config == "noshuffle",
                                        NA,
                                        resultsframe$crossvalAccuracy)

#pdf("participantPlots.pdf")
for (part in unique(resultsframe$experimentpart)) {
  for(participant in sort(unique(resultsframe$participantCode))){
    particpantaccuracyfigure <- ggplot(data = resultsframe[resultsframe$experimentpart == part &
                                                   resultsframe$participantCode == participant,],
                             aes(x = frames, y = groundtruthAccuracy, colour = config)) + geom_line() + 
      geom_line(aes(y = crossvalAccuracy), linetype = 2) +   facet_grid( facetx ~ facety) + xlab("training frames") +
      ylab("mean accuracy") + theme(legend.position = "bottom") + ggtitle(paste(participant, part)) + 
      ylim(yrange)
    # We save the figure here - it gets pulled into the document later
    #ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")
   print(particpantaccuracyfigure)
  }
}
#dev.off()
```


[yz]: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067384 "Evaluating and Improving the Depth Accuracy
of Kinect for Windows v2, L Yang et al. IEEE Sensors Journal 15(8) 2015"