---
title: "Depth data analysis"
output: html_notebook
---

## Introduction

This notebook contains details of the analysis of the (front) Kinect depth data for the spot the difference experiment.  Depth data were extracted from the Kinect xef files using the C# code.  For each Kinect frame we obtain:

* Depth (in mm) for each pixel in the depth camera's reference frame.  These data are 512x484 pixels.
* Depth (in mm) mapped to each pixel in the colour camera's reference frame.  These data are 1920x1080 pixels.  Depth data are not available for some pixels for two reasons:
     + The depth and colour camera's fields of view (and aspect ratios) are different
     + The cameras in in slightly different places, to there is a "shadow" where depth data isn't available

The "colour" depth data is obviously much larger, but contains no additional information.   In general it is easier to use the directly captured depth data rather than the mapped data. The mapped data needs to be used if we're focussing on regions of interest obtained by, e.g. Openface or CppMT.

We can "slice" the depth data to exclude the participant's surroundings, and just focus on their head and body.  The accuracy of the depth data is lower towards the edge of the frame, and appears to be noisier (TODO cite Yahng and Zahng).

I have (so far) considered two approaches to using the depth data:

* Using gaussian mixture models to look at the distribution of depths within each frame.  The mean and standard deviation of each mixture component are passed as inputs to the classifier.
    + The region of interest can be selected using either depth slicing and/or by using data from, e.g. CppMT to focus on the participant's face
* Using princpal component analysis to reduce the dimensionality of the depth data, and using the outputs from this as inputs to the classifier
    + The data can be pre-filtered by depth slicing, to exclude the participant's surroundings.

I also explore the effect of _combining_ the various tracking sources at our disposal (i.e. CppMT, OpenFace and depth PCA data)


### Gaussian mixture models

TODO writeup fully. 

Briefly... take depth readings in region of interest. Fit mixture model.   Need to fix number of components so constant for classifier.  Identifiability issues; ordering by mean not robust to large changes in behaviour.  Essentially didn't really work.


### Principal component analysis

Principal component analysis (PCA) is used to reduce the dimensionality of a data-set.  We use the incremental PCA feature of `sklearn` to extract the first 12 principal components of each frame in each experimental period.  _Incremental_ PCA allows us to batch process the depth data, and so avoid having to load all of the depth data into memory simultaneously.  (The choice of using 12 components was somewhat arbitrary; the performance of the [algorithm scales as $\mathcal{O}(\mathrm{batch size} \times \mathrm{components}^2)$](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA) )

Before performing the PCA analysis we filter each frame to only retain depths between 710 and 1710mm; values outwith this range were set to 0.  These depths were determined using the Shiny app; they retain the majority of the table, and extend as far behind the participant as possible without including the room's walls and ceiling.

The 12 components for each frame are passed to the decision tree classifier.   We repeat the analysis of the DIS2017 abstract using the PCA data instead.

```{r setup, echo=FALSE}

library(tidyverse)


getFrame <- function(participantCode, experimentpart, event, method = "extractAttention", extfile = NULL){
  
  # Get the frame number that an experiemnt part start/ends at
  part <- stringr::str_match(experimentpart, "\\d+")
  if (is.na(part)) {
    stop("Experiment part could not be extracted")
  }
  
  eventstring <- paste0(event, part)
  if (method == "extractAttention") {
    outputname <- tempfile()
    
    sysargs <- c(paste0("--attentionfile=", "../../DIS2017/Attention/", participantCode, "_attention.txt"),
                 paste0("--outputfile=", outputname),
                 paste0("--participant=", participantCode),
                 paste0("--event=", eventstring),
                 paste0("--externaleventfile=../../DIS2017/Attention/transitionAnnotations.csv")
    )
    
    system2("../../abc-display-tool/abc-extractAttention.py",
          args = sysargs,
            stdout = NULL
    )
    
    results <- read.csv(outputname)
    
    if (nrow(results) != 1) {
      stop("Error reading results frame")
    }
    
    outframe <- results[1, "frame"]
    
    unlink(outputname)
  }
  else if (method == "eventlist")
  {
    if (is.null(extfile)) {
      stop("Must specify and external file containing events and frames")
    }
    
    if (any(class(participantCode) == "data.frame")) {
      # Check we've only 1 row and if so extract code
      if (nrow(participantCode) != 1) {
        stop("Invalid participantcode data - check table only has 1 row")
      }
      participantCode <- participantCode$participantCode
    }
    
    eventdata <- read.delim(extfile, header = FALSE, 
                            col.names = c("participantCode",
                                          "event",
                                          "frame"),
                            sep = " ",
                            stringsAsFactors = FALSE)
  
    
    theevent <- eventdata[eventdata$event == eventstring & 
                            eventdata$participantCode == participantCode, ]
    
    if (nrow(theevent) != 1) {
      stop("Could not extract event")
    }
    
    outframe <- theevent$frame
  }
  
  

return(outframe)
}

getDepthName <- function(participantCode, experimentpart, depthLoc, forceWebcam = FALSE){
  # Get a filename for a depth-output file
  # We do this programatically, since we include the depths we've filtered to in the
  # filename
  
  if (forceWebcam == TRUE) { 
    possiblefiles <- list.files(path = depthLoc, pattern = "*_webcam.csv")
  } else{
    possiblefiles <- list.files(path = depthLoc, pattern = "*.csv")
  }
  exptpart <- stringr::str_match(experimentpart, "^\\w+(\\d)$")[,2]
  
  regex <- paste0("^",
                  participantCode,
                  "_",
                  "\\w+",
                  exptpart,
                  "_.+\\.csv$")
  
  filematch <- stringr::str_match(possiblefiles, regex)
  filename <- filematch[!is.na(filematch),]
  if (length(filename) != 1) {
    stop("Could not extract filename")
  }
  return(filename)
}

```




```{r}

fps <- 30

participantCode <- paste0("P",sprintf("%02d", 1:18))
numparticipants <- length(participantCode)

seqtimes <- c(15,30,60)
randframes <- c(30, 60, 120, 240, 450)
#seqtimes <- c(15)
#randframes <- c(30)
experimentpart <- c("part1", "part2")
tracker <- c("cppMT", "depthPCA", "openface")
groundTruthPath <- "../../DIS2017/Groundtruth/"
trackerPath <- "../../DIS2017/Tracking/"
PCAPath <- "PCA/"
classifier <- "../../abc-display-tool/abc-classify.py"
seqframes <- seqtimes * fps

configurations <- tidyr::crossing(participantCode, experimentpart, tracker,
                                  dplyr::bind_rows(tidyr::crossing(config = "shuffle", frames = randframes),
                                                   tidyr::crossing(config = "noshuffle", frames = seqframes))
)

#warning("TRUNCATING CONFIGS FOR DEVELOPMENT")
#configurations <- configurations[1:10,]


logConn <- file("Runlog.txt", open = "at")
resultsframe <- NULL
for (i in 1:nrow(configurations)) {
  # Generating the results can take a long while, but anything we print will end up in the paper
  # So dump the current config to a file to allow progress to be monitored
  write(paste(configurations[i,], collapse = ":"), file = "Runlog.txt", append = TRUE)
  runoutput <- tempfile()
  
  if (configurations[i, "tracker"] == "openface") {
    trackerpfn <- paste0(trackerPath, configurations[i, "participantCode"], "_front.openface.gz")
  } else if (configurations[i, "tracker"] == "cppMT") {
    trackerpfn <- paste0(trackerPath, configurations[i, "participantCode"], "_",
                        paste0(configurations[i,"experimentpart"], "_front_cppMT.csv.gz"))
  } else if (configurations[i, "tracker"] == "depthPCA") {
    trackerpfn <- paste0(PCAPath, getDepthName(configurations[i, "participantCode"],
                              configurations[i, "experimentpart"],
                              PCAPath, forceWebcam = TRUE)) 
  } else {
    stop(paste(configurations[i, "tracker"], "filename formula not yet implemented"))
  }
  
  groundtruthpfn <- paste0(groundTruthPath,
                           configurations[i, "participantCode"], "_groundtruth.csv")
  
  # Use abc-extractAttention to get start/end frame of each experiement
  # These are *webcam* frames, since we have converted the PCA tracking data to webcam reference
  startframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], 
                         "start")
  endframe <- getFrame(configurations[i,"participantCode"], configurations[i,"experimentpart"], "end")
  
  
  runargs <- c(paste0("--trackerfile=",  trackerpfn),
               paste0("--startframe=", startframe),
               paste0("--endframe=", endframe),
               paste0("--extgt=", groundtruthpfn),
               "--useexternalgt",
               paste0("--participantcode=", paste0(configurations[i,], collapse = ":")),
               paste0("--", configurations[i, "config"]),
               paste0("--summaryfile=", runoutput),
               paste0("--externaltrainingframes=", configurations[i,"frames"]),
               paste0("--rngstate=RNGstate"),
               paste0("--maxmissing=15")
  )
  

  system2(classifier,
          args = runargs,
          stdout = NULL
  )
  
  
  results <- read.csv(runoutput, header = FALSE,
                      col.names = c("configuration",
                                    "trainedframes",
                                    "startframe",
                                    "endframe",
                                    "crossvalAccuracy",
                                    "crossvalAccuracySD",
                                    "crossvalAccuracyLB",
                                    "crossvalAccuracyUB",
                                    "groundtruthAccuracy"
                      ))
  if (nrow(results) > 1) {
    stop(paste("Something went wrong for ", configurations[i,]))
  }
  resultsframe <- rbind(resultsframe,
                        data.frame(c(configurations[i,],  results)))
  unlink(runoutput)
}
close(logConn)

```

```{r}

resultsframe <- as_tibble(resultsframe)
write.csv(resultsframe, file = paste0("results", Sys.Date(), ".csv"))
resultsmean <- resultsframe %>% group_by(config, frames, tracker, experimentpart) %>%
  summarise(avgxval = mean(crossvalAccuracy),
            avggt = mean(groundtruthAccuracy),
            datapts = length(groundtruthAccuracy))
if (all(resultsmean$datapts > numparticipants)) {
  stop("Multiple experimental configs being grouped")
}
if (all(resultsmean$datapts < numparticipants)) {
  warning("Incomplete run - don't have results for all participants")
}
resultsmean$avgxval <- ifelse(resultsmean$config == "shuffle", resultsmean$avgxval, NA)
# recode levels to nicer names
conflevs <- levels(resultsmean$config)
conflevs <- ifelse(conflevs == "shuffle", "randomised", conflevs)

conflevs <- ifelse(conflevs == "noshuffle", "sequential", conflevs)
levels(resultsmean$config) <- conflevs

resultsmean <- resultsmean %>% rename(training = config)
```

```{r}
accuracyfigure <- ggplot(data = resultsmean, aes(x = frames, y = avggt, colour = training)) + geom_line() + geom_line(aes(y = avgxval), linetype = 2) +   facet_wrap( experimentpart ~ tracker) + xlab("training frames") +
  ylab("mean accuracy") + theme(legend.position = "bottom")
# We save the figure here - it gets pulled into the document later
#ggsave("figures/accuracy.pdf", plot = accuracyfigure, device = "pdf", width = 10, height = 26, units = "cm")
print(accuracyfigure)
```

We see that the PCA approach is outperformed by both the CppMT and OpenFace tracking

### Combining serveral data sources.

In this section we explore the effect of using CppMT, OpenFace and DepthPCA data on tracking accuracy.  As well as examining the accuracy of the behaviour classification, we also consider the covariates that were included in the decision tree classifier.  If covariates from one of the data sources are rarely used, this would suggest there was little benefit in including the data source.



